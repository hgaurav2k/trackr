device: cuda:0
wandb_name: "Pretrain_residuals_16ctx"
use_flash_attn: False
model:
  proprio_dim: 23
  action_dim: 23
  pc_num: 100
  hidden_dim: 192
  max_ep_len: 4096
  max_length: null
  action_tanh: false
  context_length: 16
  n_layer: 4
  n_head: 4
  use_flash_attn: True #not used
  attn_pdrop: 0.0
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  action_input: False
  scale_proprio: True
  all_fingers: False
  use_mlp: False
  full_autoregressive: True
  use_imagenet: False
  use_vit: False
  use_diffusion_policy: False 
  use_r3m: False
  use_mvp_rgb: False 
  use_r3m_depth: False
  use_vip: False
  diffusion_policy_horizon: None
  cache_all: False
  scale_action: True #these settings are for the working PolicyTransformer model
test: False
wandb_activate: False
checkpoint: ''
groundtruth_policy: '' 
load_trajectory: ''
training:
  batch_size: 512
  modality_aligned: False
  use_pc_loss: False
  use_proprio_loss: False
  num_epochs: 100
  use_residuals: False
  num_workers: 16
  dt: 0.05 # 20Hz, control frequency to scale the action (if activated)
  lr: 0.0001
  add_proprio_noise: True
  add_action_noise: True
  noise_arm: 0.1
  noise_hand: 0.1
  add_data_driven_noise: False
  weight_decay: 0.01
  log_freq: 1000
  model_save_freq: 1000
  time_shift: 0
  model_save_dir: algo/pretrained/models
  root_dir: retarget_data/train
  load_checkpoint: False 
  checkpoint_path: ''
validation:
  root_dir: retarget_data/val
