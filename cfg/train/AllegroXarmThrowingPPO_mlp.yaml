seed: ${..seed}
algo: PPO
network:
  mlp:
    units: [512, 256, 128]
  priv_mlp:
    units: [256, 128, 8]

  pc_mlp:
    out_dim: 64
    units: [64,64]

load_path: ${..checkpoint} # path to the checkpoint to load

ppo:
  output_name: 'debug'
  normalize_input: True
  normalize_value: True
  normalize_pc: False
  normalize_proprio_hist: False
  value_bootstrap: True
  num_actors: ${...task.env.numEnvs}
  num_gradient_steps: ${...train.ppo.horizon_length}
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  kl_threshold: 0.02
  learning_rate: 5e-3
  min_lr: 1e-6
  max_lr: 5e-3
  # PPO batch collection
  horizon_length: 10
  minibatch_size: 32768
  mini_epochs: 5
  # PPO loss setting
  clip_value: True
  critic_coef: 4
  entropy_coef: 0.0
  e_clip: 0.2
  bounds_loss_coef: 0.0001
  # grad clipping
  truncate_grads: True
  grad_norm: 1.0
  # snapshot setting
  save_best_after: 0
  save_frequency: 100
  max_agent_steps: 5000000000
  # hora setting
  priv_info: False
  priv_info_dim: 9
  priv_info_embed_dim: 8
  proprio_adapt: False
  dapg:
    l1: 0.1 
    l2: 0.999
    dapg_threshold: 0.002