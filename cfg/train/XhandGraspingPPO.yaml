# params:
#   seed: ${...seed}

#   algo:
#     name: a2c_continuous

#   model:
#     name: continuous_a2c_logstd

#   network:
#     name: a2c_pointnet
#     separate: False

#     space:
#       continuous:
#         mu_activation: None
#         sigma_activation: None
#         mu_init:
#           name: default
#         sigma_init:
#           name: const_initializer
#           val: 0
#         fixed_sigma: True

#     mlp:
#       units: [1024, 1024, 512, 512]
#       activation: elu
#       d2rl: False
#       initializer:
#         name: default
#       regularizer:
#         name: None

#   load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
#   load_path: ${...checkpoint} # path to the checkpoint to load

#   config:
#     name: ${resolve_default:AllegroKukaPPO,${....experiment}}
# #    full_experiment_name: ${.name}
#     env_name: rlgpu
#     multi_gpu: ${....multi_gpu}
#     ppo: True
#     mixed_precision: True
#     normalize_input: True
#     normalize_value: True
#     normalize_advantage: True
#     reward_shaper:
#       scale_value: 0.01

#     num_actors: ${....task.env.numEnvs}   
#     gamma: 0.99
#     tau: 0.95
#     learning_rate: 1e-4
#     lr_schedule: adaptive
#     schedule_type: standard
#     kl_threshold: 0.016
#     score_to_win: 1000000
#     max_epochs: 100000
#     max_frames: 10_000_000_000
#     save_best_after: 100
#     save_frequency: 5000
#     print_stats: True
#     grad_norm: 1.0
#     entropy_coef: 0.0
#     truncate_grads: True
#     e_clip: 0.1
#     minibatch_size: 8192
#     mini_epochs: 4
#     critic_coef: 4.0
#     clip_value: True
#     horizon_length: 16
#     seq_length: 16

#     # SampleFactory currently gives better results without bounds loss but I don't think this loss matters too much
#     # bounds_loss_coef: 0.0
#     bounds_loss_coef: 0.0001

#     # optimize summaries to prevent tf.event files from growing to gigabytes
#     defer_summaries_sec: 5
#     summaries_interval_sec_min: 5
#     summaries_interval_sec_max: 300

#     player:
#       #render: True
#       deterministic: False  # be careful there's a typo in older versions of rl_games in this parameter name ("determenistic")
#       games_num: 100000
#       print_stats: False
seed: ${..seed}
algo: PPOTransformer
network:
  proprio_dim: 23
  action_dim: 23
  pc_num: 100
  hidden_dim: 192
  max_ep_len: 4096
  max_length: null
  action_tanh: false
  context_length: 16
  n_layer: 4
  n_head: 4
  use_flash_attn: True #not used
  attn_pdrop: 0.0
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  action_input: False
  scale_proprio: True
  full_autoregressive: True
  use_imagenet: False
  use_vit: False
  use_diffusion_policy: False 
  use_r3m: False
  use_mvp_rgb: False 
  use_r3m_depth: False
  use_vip: False
  diffusion_policy_horizon: None
  cache_all: False
  scale_action: True #these settings are for the working PolicyTransformer model

load_path: ${..checkpoint} # path to the checkpoint to load

ppo:
  output_name: 'debug'
  normalize_input: True
  normalize_value: True
  normalize_pc: False
  normalize_proprio_hist: False
  value_bootstrap: True
  num_actors: ${...task.env.numEnvs}
  num_gradient_steps: ${...train.ppo.horizon_length}
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  initEpsArm: 1.0
  initEpsHand: 1.0
  value_grads_to_pointnet: True
  point_cloud_input_to_value: True
  learning_rate: 1e-4
  kl_threshold: 0.02
  min_lr: 1e-6
  max_lr: 1e-4
  # PPO batch collection
  horizon_length: 10
  minibatch_size: 4096
  mini_epochs: 1
  # PPO loss setting
  clip_value: True
  critic_coef: 4
  entropy_coef: 0.0
  e_clip: 0.2
  bounds_loss_coef: 0.0001
  # grad clipping
  truncate_grads: True
  grad_norm: 1.0
  # snapshot setting
  save_best_after: 0
  save_frequency: 1250
  max_agent_steps: 5000000000
  critic_warmup_steps: -1
  # hora setting
  priv_info: False
  priv_info_dim: 9
  priv_info_embed_dim: 8
  proprio_adapt: False
  useMemoryEfficientBuffer: False
  dapg:
    l1: 0.1 
    l2: 0.999
    dapg_threshold: 0.002

wandb:
  activate: True
  entity: himanshu_singh
  project: grasping
