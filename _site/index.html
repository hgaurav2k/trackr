<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    Himanshu Gaurav Singh
  
</title>
<meta name="author" content="Himanshu Gaurav Singh">
<meta name="description" content="Leveraging visual understanding of motion for policy learning">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/">


  <!-- Dark Mode -->
  <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>
  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>



  <!-- GeoJSON support via Leaflet -->
  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous">



  <!-- diff2html -->
  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)">
  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)">
  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous">





  <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">



    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <style>
      /* Improve caption contrast in dark mode */
      @media (prefers-color-scheme: dark) {
        .post.distill figcaption.caption,
        .post.distill .figure-caption {
          color: #e6e6e6 !important;
          opacity: 1 !important;
        }
      }
      body.dark .post.distill figcaption.caption,
      body.dark .post.distill .figure-caption {
        color: #e6e6e6 !important;
        opacity: 1 !important;
      }
    </style>
    
      <!-- Page/Post style -->
      <style type="text/css">
        .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

      </style>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Deep Sensorimotor Control by Imitating Predictive Models of Human Motion",
            "description": "Leveraging visual understanding of motion for policy learning",
            "published": "May 22, 2021",
            "authors": [
              
              {
                "author": "Himanshu Gaurav Singh",
                "authorURL": "https://hgaurav2k.github.io"
              },
              
              {
                "author": "Pieter Abbeel",
                "authorURL": "https://people.eecs.berkeley.edu/~pabbeel/"
              },
              
              {
                "author": "Jitendra Malik",
                "authorURL": "https://people.eecs.berkeley.edu/~malik/"
              },
              
              {
                "author": "Antonio Loquercio",
                "authorURL": "https://antonilo.github.io"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->

    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Deep Sensorimotor Control by Imitating Predictive Models of Human Motion</h1>
        <p>Leveraging visual understanding of motion for policy learning</p>
        
          
          
            
          
            
          
            
          
            
          
          
            <p class="author-names" style="font-size: 0.95rem; color: #666; margin-top: 1rem; font-family: Arial, sans-serif;">
              
                
                  <a href="https://hgaurav2k.github.io" style="color: inherit; text-decoration: underline; text-underline-offset: 2px;" rel="external nofollow noopener" target="_blank">Himanshu Gaurav Singh</a>
                
                  
              
                
                  <a href="https://people.eecs.berkeley.edu/~pabbeel/" style="color: inherit; text-decoration: underline; text-underline-offset: 2px;" rel="external nofollow noopener" target="_blank">Pieter Abbeel</a>
                
                  
              
                
                  <a href="https://people.eecs.berkeley.edu/~malik/" style="color: inherit; text-decoration: underline; text-underline-offset: 2px;" rel="external nofollow noopener" target="_blank">Jitendra Malik</a>
                
                  
              
                
                  <a href="https://antonilo.github.io" style="color: inherit; text-decoration: underline; text-underline-offset: 2px;" rel="external nofollow noopener" target="_blank">Antonio Loquercio</a>
                
                
              
            </p>
          
        
      </d-title>
      
      
      <div class="resource-links" style="margin: 0.25rem 0 1rem 0; display: flex; gap: 0.75rem; flex-wrap: wrap; justify-content: center;">
        
          <a href="https://github.com/hgaurav2k/trackr" target="_blank" rel="noopener" style="display: inline-block; padding: 10px 18px; border-radius: 10px; background: #0d6efd; color: #fff; text-decoration: none; font-size: 1rem; font-weight: 600;">Code</a>
        
        
          <a href="assets/tracking_predictive_models.pdf" target="_blank" rel="noopener" style="display: inline-block; padding: 10px 18px; border-radius: 10px; background: #0d6efd; color: #fff; text-decoration: none; font-size: 1rem; font-weight: 600;">Paper</a>
        
      </div>
      

      <d-article>
        
        <p>Every day, terabytes of video capture how humans interact with the world — opening doors, cooking meals, stacking boxes. This data gives away a lot of detail about what we want our robots to do. The guiding question in this project is: <em>What is the most scalable way to utilize this data for robot learning?</em></p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/intro-480.webp 480w,/assets/intro-800.webp 800w,/assets/intro-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/intro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Placeholder figure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

<p>The most common paradigm for learning from human data is <em>real2sim</em>, which seeks to replicate the environment in which humans operate by estimating its geometry and physical parameters. The resulting digital twin is necessary to map human motion to robot motion in the same scene as the video. In its full generality, this is a very hard inverse problem to solve. While working on HOP <d-cite key="hop"></d-cite>, we felt bottlenecked by the performance of existing solutions for this problem.</p>

<p>This work presents an approach to circumvent these methods all together. We seek to unify all human video data into one neural network and subsequently use it for policy learning. Specifically, we train a model to predict future human keypoint trajectory given the current scene and task and then use it as a <em>reward model</em>. Such a reward model incentivizes a robot to <em>track</em> the motion of humans’ end effectors (hands!), given the observations and task. This learned reward, combined with a simple, sparse task-specific reward, is then used to train sensorimotor policies via massively parallelized reinforcement learning in simulation.</p>

<p>As the human dataset grows, we expect the reward model to capture human motion for a lot of diverse tasks and scenes. The knowledge encoded in this model enables large-scale policy training with reinforcement learning by eliminating the need for per-task manual reward design. Indeed, as demonstrated in our experiments, <u>a single reward model suffices to train policies across diverse tasks and robot embodiments.</u></p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/approach-480.webp 480w,/assets/approach-800.webp 800w,/assets/approach-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/approach.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Placeholder figure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
    <figcaption class="caption" style="text-align: left; font-size: 0.9em;">We use a dataset of humans interacting with their scene to train a motion prediction model $\Pi_h$. Such model, instantiated as a causal transformer, takes as input a history of previous 3D keypoints $k^h_{t:t-L}$, i.e., the location of the human's fingertips, and observations $o_{t:t-L}$, i.e., the objects' pointcloud, to predict the future human keypoint location $\hat{k}^h_{t+1}$. For anthropomorphic robots, thanks to the abstraction of keypoints, $\Pi_h$ can be used on robot data despite being trained on human data. Therefore, $\Pi_h$ can predict likely human motions while training a policy $\pi_{\theta}$ on a downstream task. A reward to track such motions, $r_{track}$, provides an additional training signal to the otherwise sparse task reward $r_{task}$.</figcaption>
  
</figure>

<h2 id="the-reward-model">The reward model</h2>
<p>The goal of the reward model is to guide robot policies to behave similarly to humans in the robot’s environment. This reward model is trained on human data, but tested on robot data. For this to happen zero-shot, we need an abstraction level at which robot motion and human motion are as similar as possible. For anthropomorphic robots, there is a simple and intuitive choice: the trajectories of fingertips.</p>

<p>We set up our human motion model \(\Pi_{h}\) to predict the most likely future fingertips location given a history of previous fingertip positions and scene observations (Fig. X). Practically, in our implementation \(\Pi_{h}\) is a causal transformer trained on a dataset \(\mathcal{D} := {g^{(i)}, (k^{h(i)}_1, o^{(i)}_1, k^{h(i)}_2, o^{(i)}_2, \dots)}_{i=1}^n,\)</p>

<p>which consists of trajectories of fingertips \(k^{h}_t\) and object point cloud observations \(o_t\), as well as a goal label \(g\). This dataset contains instances of humans interacting with their surroundings to accomplish the task \(g\).</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/keypoints-480.webp 480w,/assets/keypoints-800.webp 800w,/assets/keypoints-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/keypoints.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Placeholder figure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
    <figcaption class="caption" style="text-align: left; font-size: 0.9em;">Anthropomorphic hands allow an intuitive mapping of robot links to human hand keypoints. It is defined once and remains consistent across tasks. Here, we show the mapping of the human hand to three different morphologies: an Allegro hand (left), an Xhand (center), and an SVH hand (right).</figcaption>
  
</figure>

<h2 id="training-robot-policies-pi_h-as-general-reward">Training robot policies: \(\Pi_{h}\) as general reward</h2>

<figure class="figure">
<div class="row" style="margin-bottom: 0.25rem;">
    <div class="col-sm">
        

<figure>
  
    <video src="/assets/tracks-1.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video>

  
  
</figure>

    </div>
    <div class="col-sm">
        

<figure>
  
    <video src="/assets/tracks-2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video>

  
  
</figure>

    </div>
</div>
<figcaption class="figure-caption text-left" style="margin-top: 0.1rem;">Rollouts of a policy trained with our approach. Inputs to the policy are the point cloud of the object and the proprioceptive state of the robot. Also visualized are predictions of $\Pi_{h}$ conditioned on the point cloud and keypoint trajectory of the robot till the current timestep. We find that $\Pi_{h}$ guides the agent to a human-like grasp pose with a smooth pre-grasp trajectory. More qualitative results can be found <a href="https://jirl-upenn.github.io/track_reward" rel="external nofollow noopener" target="_blank">here</a></figcaption>
</figure>
<p>Massively parallelized reinforcement learning in simulation offers the potential to train powerful and general-purpose robot policies. Yet, current RL pipelines still require extensive manual intervention, particularly in two areas: (i) environment design and (ii) reward design. Both are challenging, and as a result, the most impressive successes to date have been limited to task-specific settings. Achieving greater generality in policy learning will require automating these components. While recent work has begun to address environment design by, e.g., using VLMs for scene generation, reward design remains largely manual (with only limited support provided by VLMs).</p>

<p>Tracking human motion is a fairly general way to specify behavior and, therefore, a viable solution to automate reward design. This is strictly more expressive than coding up a reward, in which the case the reward function is limited to analytic expressions of the available state variables. To demonstrate the applicability of our approach, we use \(\Pi_H\) as a <em>general</em> reward for multiple tasks and robot embodiments. Empirically, we find that training policies with rewards from \(\Pi_H\) almost matches the performance of doing RL with hand-engineered rewards (PPO-Dense).</p>

<p>Note that we couple \(\Pi_H\) with a robot-independent sparse task reward, measuring, for example, whether the desired object has arrived at the target location. Such a task reward is insufficient for policy training, as the performance of PPO-Sparse shows in the plot below.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/fig5_cabinet-480.webp 480w,/assets/fig5_cabinet-800.webp 800w,/assets/fig5_cabinet-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/fig5_cabinet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Placeholder figure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
    <figcaption class="caption" style="text-align: left; font-size: 0.9em;">Tracking the predictions of $\Pi_{h}$ enables tractable reinforcement learning with a sparse task reward. Our approach is comparable to the privileged baseline: *PPO with dense rewards* across all tasks, whereas PPO with the sparse task reward only fails to learn.</figcaption>
  
</figure>

<h2 id="what-next">What next</h2>

<figure class="figure">
<div class="row" style="margin-bottom: 0.25rem;">
    <div class="col-sm" title="Pick up the red crab from the gray tray and place it into the blue bowl">
        

<figure>
  
    <video src="/assets/compressed/28.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video>

  
  
</figure>

    </div>
    <div class="col-sm" title="Pick up the yellow banana from the wooden table with the right hand and place it in the gray bowl">
        

<figure>
  
    <video src="/assets/compressed/167.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video>

  
  
</figure>

    </div>
</div>
<div class="row" style="margin-bottom: 0.25rem;">
    <div class="col-sm" title="Pick up a grey iPhone from the metal table and place it on the box lid">
        

<figure>
  
    <video src="/assets/compressed/1795.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video>

  
  
</figure>

    </div>
    <div class="col-sm" title="Pick up the blue tongs from the table with the left hand and place them into the tupperware">
        

<figure>
  
    <video src="/assets/compressed/13003.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video>

  
  
</figure>

    </div>
</div>
<figcaption class="figure-caption text-left" style="margin-top: 0.1rem;"><strong>Keypoint prediction with RGB input</strong> We train a keypoint prediction model on the EgoDex<d-cite key="egodex"></d-cite> dataset conditioned on RGB image, language description of the task and current keypoints. Predicted future hand keypoints are marked in red. Language descriptions can be viewed by hovering the pointer on the corresponding video. Training such a model on more diverse videos and tasks can potentially provide a <strong>general reward model</strong> that generalizes to new scenes.</figcaption>
</figure>

<p>Our results are at the moment a proof-of-concept. Arguably, why bother training predictive models if we can design a dense reward? The real power of our approach comes with scaling up \(\Pi_h\) with internet-scale human data (e.g., videos or VR/AR applications), using  RGB/RGB-D images as observations and language as goal description. This will unlock a truly general reward for arbitrary tasks and scenes.</p>

<p>From the modeling perspective, learning to predict changes in the environment and not just the human motion will make the reward more expressive, possibly removing the need for any other sparse reward for RL. However, that basically implies training a <em>world model</em> in RGB space, which is beyond our means at the moment. As open-source video models improve in quality, an approach to cheaply re-purpose them as a general reward model is a very promising direction for future work.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Viser, by Brent Yi, has been very helpful in designing custom remote visualization tools for analysis and debugging throughout this project. We would like to thank Arthur Allshire, Ankur Handa and  Jathushan Rajasegaran for helpful discussions during the course of the project.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography>
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  © Copyright 2025
  Himanshu
  Gaurav
  Singh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

  
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

<!-- Custom overrides -->
<script src="/assets/js/distillpub/overrides.js"></script>


  <!-- Mermaid and D3 -->
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script>
  
    <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script>
  
  <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script>



  <!-- Diff2HTML -->
  <!-- diff2html doesn't go well with Bootstrap Table -->
  <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script>



  <!-- Leaflet -->
  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script>



  <!-- Chart.js -->
  <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script>



  <!-- ECharts -->
  <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script>
  
  <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script>





  <!-- Vega -->
  <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script>



  <!-- Tikzjax -->
  <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script>



  <!-- Typograms -->
  <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script>





  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Removed Badges -->


  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
  <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  <!-- Removed Pseudocode -->









  <!-- Scrolling Progress Bar -->
  <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/assets/js/search-data.js"></script>
  <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>


  
</body>
</html>
