---
layout: distill
title: Deep Sensorimotor Control by Imitating Predictive Models of Human Motion
description: Leveraging visual understanding of motion for policy learning
tags: distill formatting
giscus_comments: true
date: 2021-05-22
featured: true
code_url: https://github.com/hgaurav2k/trackr
arxiv_url: https://github.com/hgaurav2k/trackr
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true
permalink: /



authors:
  - name: Himanshu Gaurav Singh
    url: "https://hgaurav2k.github.io"
  - name: Pieter Abbeel
    url: https://people.eecs.berkeley.edu/~pabbeel/
  - name: Jitendra Malik
    url: https://people.eecs.berkeley.edu/~malik/
  - name: Antonio Loquercio
    url: "https://antonilo.github.io"

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).


# toc:
#   - name: Equations
#   - name: Citations
#   - name: Footnotes
#   - name: Code Blocks
#   - name: Interactive Plots
#   - name: Mermaid
#   - name: Diff2Html
#   - name: Leaflet
#   - name: Chartjs, Echarts and Vega-Lite
#   - name: TikZ
#   - name: Typograms
#   - name: Layouts
#   - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }


---
  Every day, terabytes of video capture how humans interact with the world — opening doors, cooking meals, stacking boxes. This data gives away a lot of detail about what we want our robots to do. The guiding question in this project is: *What is the most scalable way to utilize this data for robot learning?*

{% include figure.liquid path="assets/intro.png" class="img-fluid rounded z-depth-1" zoomable=true alt="Placeholder figure"%}



  The most common paradigm for learning from human data is *real2sim*, which seeks to replicate the environment in which humans operate by estimating its geometry and physical parameters. The resulting digital twin is necessary to map human motion to robot motion in the same scene as the video. In its full generality, this is a very hard inverse problem to solve. While working on HOP <d-cite key="hop"></d-cite>, we felt bottlenecked by the performance of existing solutions for this problem.

  This work presents an approach to circumvent these methods all together. We seek to unify all human video data into one neural network and subsequently use it for policy learning. Specifically, we train a model to predict future human keypoint trajectory given the current scene and task and then use it as a *reward model*. Such a reward model incentivizes a robot to *track* the motion of humans' end effectors (hands!), given the observations and task. This learned reward, combined with a simple, sparse task-specific reward, is then used to train sensorimotor policies via massively parallelized reinforcement learning in simulation.


  As the human dataset grows, we expect the reward model to capture human motion for a lot of diverse tasks and scenes. The knowledge encoded in this model enables large-scale policy training with reinforcement learning by eliminating the need for per-task manual reward design. Indeed, as demonstrated in our experiments, <u>a single reward model suffices to train policies across diverse tasks and robot embodiments.</u>
{% include figure.liquid path="assets/approach.png" class="img-fluid rounded z-depth-1" zoomable=true alt="Placeholder figure" caption="We use a dataset of humans interacting with their scene to train a motion prediction model $\Pi_h$. Such model, instantiated as a causal transformer, takes as input a history of previous 3D keypoints $k^h_{t:t-L}$, i.e., the location of the human's fingertips, and observations $o_{t:t-L}$, i.e., the objects' pointcloud, to predict the future human keypoint location $\hat{k}^h_{t+1}$. For anthropomorphic robots, thanks to the abstraction of keypoints, $\Pi_h$ can be used on robot data despite being trained on human data. Therefore, $\Pi_h$ can predict likely human motions while training a policy $\pi_{\theta}$ on a downstream task. A reward to track such motions, $r_{track}$, provides an additional training signal to the otherwise sparse task reward $r_{task}$." caption-align="left" %}

## The reward model
The goal of the reward model is to guide robot policies to behave similarly to humans in the robot’s environment. This reward model is trained on human data, but tested on robot data. For this to happen zero-shot, we need an abstraction level at which robot motion and human motion are as similar as possible. For anthropomorphic robots, there is a simple and intuitive choice: the trajectories of fingertips. 

We set up our human motion model $$\Pi_{h}$$ to predict the most likely future fingertips location given a history of previous fingertip positions and scene observations (Fig. X). Practically, in our implementation $$\Pi_{h}$$ is a causal transformer trained on a dataset $$\mathcal{D} := {g^{(i)}, (k^{h(i)}_1, o^{(i)}_1, k^{h(i)}_2, o^{(i)}_2, \dots)}_{i=1}^n,$$ 

which consists of trajectories of fingertips $$k^{h}_t$$ and object point cloud observations $$o_t$$, as well as a goal label $$g$$. This dataset contains instances of humans interacting with their surroundings to accomplish the task $$g$$.
{% include figure.liquid path="assets/keypoints.png" class="img-fluid rounded z-depth-1" zoomable=true alt="Placeholder figure" caption="Anthropomorphic hands allow an intuitive mapping of robot links to human hand keypoints. It is defined once and remains consistent across tasks. Here, we show the mapping of the human hand to three different morphologies: an Allegro hand (left), an Xhand (center), and an SVH hand (right)." caption-align="left" %}

## Training robot policies: $$\Pi_{h}$$ as general reward

<figure class="figure">
<div class="row" style="margin-bottom: 0.25rem;">
    <div class="col-sm">
        {% include video.liquid path="assets/tracks-1.mp4" class="img-fluid rounded z-depth-1" autoplay=true loop=true muted=true %}
    </div>
    <div class="col-sm">
        {% include video.liquid path="assets/tracks-2.mp4" class="img-fluid rounded z-depth-1" autoplay=true loop=true muted=true %}
    </div>
</div>
<figcaption class="figure-caption text-left" style="margin-top: 0.1rem;">Rollouts of a policy trained with our approach. Inputs to the policy are the point cloud of the object and the proprioceptive state of the robot. Also visualized are predictions of $\Pi_{h}$ conditioned on the point cloud and keypoint trajectory of the robot till the current timestep. We find that $\Pi_{h}$ guides the agent to a human-like grasp pose with a smooth pre-grasp trajectory.</figcaption>
</figure>
Massively parallelized reinforcement learning in simulation offers the potential to train powerful and general-purpose robot policies. Yet, current RL pipelines still require extensive manual intervention, particularly in two areas: (i) environment design and (ii) reward design. Both are challenging, and as a result, the most impressive successes to date have been limited to task-specific settings. Achieving greater generality in policy learning will require automating these components. While recent work has begun to address environment design by, e.g., using VLMs for scene generation, reward design remains largely manual (with only limited support provided by VLMs).

Tracking human motion is a fairly general way to specify behavior and, therefore, a viable solution to automate reward design. This is strictly more expressive than coding up a reward, in which the case the reward function is limited to analytic expressions of the available state variables. To demonstrate the applicability of our approach, we use $$\Pi_H$$ as a *general* reward for multiple tasks and robot embodiments. Empirically, we find that training policies with rewards from $$\Pi_H$$ almost matches the performance of doing RL with hand-engineered rewards (PPO-Dense).

Note that we couple $$\Pi_H$$ with a robot-independent sparse task reward, measuring, for example, whether the desired object has arrived at the target location. Such a task reward is insufficient for policy training, as the performance of PPO-Sparse shows in the plot below.

{% include figure.liquid path="assets/fig5_cabinet.png" class="img-fluid rounded z-depth-1" zoomable=true alt="Placeholder figure" caption="Tracking the predictions of $\Pi_{h}$ enables tractable reinforcement learning with a sparse task reward. Our approach is comparable to the privileged baseline: *PPO with dense rewards* across all tasks, whereas PPO with the sparse task reward only fails to learn." caption-align="left" %}

## What next

<figure class="figure">
<div class="row" style="margin-bottom: 0.25rem;">
    <div class="col-sm" title="Pick up the red crab from the gray tray and place it into the blue bowl">
        {% include video.liquid path="assets/compressed/28.mp4" class="img-fluid rounded z-depth-1" autoplay=true loop=true muted=true %}
    </div>
    <div class="col-sm" title="Pick up the yellow banana from the wooden table with the right hand and place it in the gray bowl">
        {% include video.liquid path="assets/compressed/167.mp4" class="img-fluid rounded z-depth-1" autoplay=true loop=true muted=true %}
    </div>
</div>
<div class="row" style="margin-bottom: 0.25rem;">
    <div class="col-sm" title="Pick up a grey iPhone from the metal table and place it on the box lid">
        {% include video.liquid path="assets/compressed/1795.mp4" class="img-fluid rounded z-depth-1" autoplay=true loop=true muted=true %}
    </div>
    <div class="col-sm" title="Pick up the blue tongs from the table with the left hand and place them into the tupperware">
        {% include video.liquid path="assets/compressed/13003.mp4" class="img-fluid rounded z-depth-1" autoplay=true loop=true muted=true %}
    </div>
</div>
<figcaption class="figure-caption text-left" style="margin-top: 0.1rem;"><strong>Keypoint prediction with RGB input</strong> We train a keypoint prediction model on the EgoDex<d-cite key="egodex"></d-cite> dataset conditioned on RGB image, language description of the task and current keypoints. Predicted future hand keypoints are marked in red. Language descriptions can be viewed by hovering the pointer on the corresponding video. Training such a model on more diverse videos and tasks can potentially provide a <strong>general reward model</strong> that generalizes to new scenes.</figcaption>
</figure>

Our results are at the moment a proof-of-concept. Arguably, why bother training predictive models if we can design a dense reward? The real power of our approach comes with scaling up $$\Pi_h$$ with internet-scale human data (e.g., videos or VR/AR applications), using  RGB/RGB-D images as observations and language as goal description. This will unlock a truly general reward for arbitrary tasks and scenes.


From the modeling perspective, learning to predict changes in the environment and not just the human motion will make the reward more expressive, possibly removing the need for any other sparse reward for RL. However, that basically implies training a *world model* in RGB space, which is beyond our means at the moment. As open-source video models improve in quality, an approach to cheaply re-purpose them as a general reward model is a very promising direction for future work.

## Acknowledgements

Viser, by Brent Yi, has been very helpful in designing custom remote visualization tools for analysis and debugging throughout this project. We would like to thank Arthur Allshire, Ankur Handa and  Jathushan Rajasegaran for helpful discussions during the course of the project.

